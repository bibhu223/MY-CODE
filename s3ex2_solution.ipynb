{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train word2vec from scratch\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Model, Sequential \n",
                "from tensorflow.keras.layers import Input, Dense, Reshape,Embedding,dot\n",
                "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "from tensorflow.keras.preprocessing import sequence\n",
                "from helper import build_dataset\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For this example, let us consider the sentence from the lecture \n",
                "raw_data = \"Ignacio was hit by a red bus a while ago\"\n",
                "corpus = raw_data.split()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We define the parameters for our skipgram with negative sampling \n",
                "window_size = 2\n",
                "vector_dim = 300\n",
                "vocab_size = len(set(corpus))+1\n",
                "\n",
                "# Use the helper function to convert the corpus to sequential data\n",
                "data, count, dictionary, reverse_dictionary = build_dataset(corpus,vocab_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[1, 6], [8, 9], [6, 5], [7, 1], [5, 5]] [1, 1, 1, 1, 0]\n"
                }
            ],
            "source": [
                "# We use the `skipgrams` function from tensorflow.keras\n",
                "# to build the training dataset\n",
                "couples, labels = skipgrams(data, vocab_size, window_size=window_size)\n",
                "word_target, word_context = zip(*couples)\n",
                "# word_target = np.array(word_target, dtype=\"int32\")\n",
                "# word_context = np.array(word_context, dtype=\"int32\")\n",
                "print(couples[:5], labels[:5])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ‚è∏ Which number from the `data` variable corresponds to **Ignacio**?\n",
                "\n",
                "\n",
                "#### A. 5\n",
                "#### B. 2\n",
                "#### C. [1,0,0,0,0,1]\n",
                "#### D. [0,1,0,0,0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow1) ###\n",
                "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
                "answer1 = 'B'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Word2Vec Skipgram with Negative sampling Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"SGNS\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nembedding_input (InputLayer)    [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ncontext_input (InputLayer)      [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 1, 300)       3000        embedding_input[0][0]            \n__________________________________________________________________________________________________\ncontext (Embedding)             (None, 1, 300)       3000        context_input[0][0]              \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 300)          0           embedding[0][0]                  \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 300)          0           context[0][0]                    \n__________________________________________________________________________________________________\ndotproduct (Dot)                (None, 1)            0           reshape[0][0]                    \n                                                                 reshape_1[0][0]                  \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 1)            2           dotproduct[0][0]                 \n==================================================================================================\nTotal params: 6,002\nTrainable params: 6,002\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
                }
            ],
            "source": [
                "# The following code builds the SGNS word2vec architecture\n",
                "\n",
                "# Here we build the target word embedding\n",
                "word_model = Sequential() \n",
                "word_model.add(Embedding(vocab_size, vector_dim, \n",
                "               input_length=1,name='embedding')) \n",
                "word_model.add(Reshape((vector_dim, ))) \n",
                "\n",
                "# Here we build the context word embedding\n",
                "context_model = Sequential() \n",
                "context_model.add(Embedding(vocab_size, vector_dim, \n",
                "                  input_length=1,name='context')) \n",
                "context_model.add(Reshape((vector_dim,))) \n",
                "\n",
                "# Here we take the dot product of the the target and context word\n",
                "\n",
                "dot_product = dot([word_model.output, context_model.output], axes=1,\n",
                "                  normalize=False,name='dotproduct') \n",
                "dot_product = Dense(1,activation=\"sigmoid\")(dot_product) \n",
                "\n",
                "# We use the functional API to bring all the above parts together\n",
                "# to complete the word2vec architecture\n",
                "model = Model(inputs=[word_model.input, context_model.input], \n",
                "              outputs=dot_product,name='SGNS') \n",
                "# We run the model summary to see the full architecture \n",
                "model.summary()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ‚è∏ What are the dimensions of the embedding matrix for the above model?\n",
                "\n",
                "\n",
                "#### A. $(300,100)$\n",
                "#### B. $(10,300)$\n",
                "#### C. $(300,1)$\n",
                "#### D. $(300,10)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow2) ###\n",
                "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
                "answer1 = 'B'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We compile the model using binary crossentropy and rmsprop optimizer\n",
                "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\") "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Loss after one epoch is 0.62\n"
                }
            ],
            "source": [
                "# Lets choose a random training sample\n",
                "idx = np.random.randint(0, len(labels)-1)\n",
                "\n",
                "# Using the index we call the input values \n",
                "# NOTE: we process the input to comply with the model\n",
                "# i.e changing dtype and shape\n",
                "target_input = np.array(word_target[idx],dtype='float32').reshape(1,)\n",
                "context_input = np.array(word_context[idx],dtype='float32').reshape(1,)\n",
                "training_label = np.array(labels[idx],dtype='float32').reshape(1,)\n",
                "\n",
                "# We use the tf.keras `model.train_on_batch` to train on a single batch\n",
                "# for demonstration that our model works\n",
                "loss = model.train_on_batch([target_input, context_input], training_label)\n",
                "print(f'Loss after one epoch is {loss:.2f}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Rebuilding the SGNS word2vec architecture without the *embedding layer*"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ‚è∏ What does the `tf.keras.layers.Embedding` do?\n",
                "\n",
                "\n",
                "#### A. It makes a dictionary with keys as input and values as weights\n",
                "#### B. It converts an input of type `str` to type `float`\n",
                "#### C. It one-hot encode the input and adds an appropriate dense layer\n",
                "#### D. It creates $n$-dimension list for embedding size of $(n,)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow3) ###\n",
                "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
                "answer1 = 'C'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"Custom\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 1, 10)]      0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 1, 10)]      0                                            \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1, 300)       3000        input_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1, 300)       3000        input_2[0][0]                    \n__________________________________________________________________________________________________\nreshape_2 (Reshape)             (None, 300)          0           dense_1[0][0]                    \n__________________________________________________________________________________________________\nreshape_3 (Reshape)             (None, 300)          0           dense_2[0][0]                    \n__________________________________________________________________________________________________\ndotproduct (Dot)                (None, 1)            0           reshape_2[0][0]                  \n                                                                 reshape_3[0][0]                  \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 1)            2           dotproduct[0][0]                 \n==================================================================================================\nTotal params: 6,002\nTrainable params: 6,002\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
                }
            ],
            "source": [
                "# We build the sub-model for target words\n",
                "# As a dense layer on a one-hot encoded input without bias term\n",
                "# remember that the dense layer will have activation 'linear'\n",
                "# and number of neurons as the embedding dimension\n",
                "word_model = Sequential() \n",
                "word_model.add(Input(shape=(1,vocab_size)))\n",
                "word_model.add(Dense(vector_dim,activation='linear',use_bias=False))\n",
                "word_model.add(Reshape((vector_dim, ))) \n",
                "\n",
                "# We build the same for the context words\n",
                "context_model = Sequential() \n",
                "context_model.add(Input(shape=(1,vocab_size)))\n",
                "context_model.add(Dense(300,activation='linear',use_bias=False))\n",
                "context_model.add(Reshape((vector_dim, ))) \n",
                "\n",
                "# We use the `tf.keras.layers.dot` which returns the \n",
                "# dot product of two output vectors\n",
                "dot_product = dot([word_model.output, context_model.output], axes=1,\n",
                "                  normalize=False,name='dotproduct') \n",
                "\n",
                "# We also add a sigmoid to ensure the outputs are between 0 \u0026 1\n",
                "dot_product = Dense(1,activation=\"sigmoid\")(dot_product)\n",
                "\n",
                "# Similar to the model above we create our model with inputs\n",
                "# from `word_model` and `context_model` and the output from \n",
                "# the `dot_product`\n",
                "model = Model(inputs=[word_model.input, context_model.input], \n",
                "              outputs=dot_product,name='Custom') \n",
                "\n",
                "# Again we run the model summary to ensure we have built the\n",
                "# word2vec architecture correctly\n",
                "# Note that you must have 6,002 trainable parameters\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Again we compile the model using binary crossentropy and rmsprop optimizer\n",
                "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\") "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Loss after one epoch is 0.74\n"
                }
            ],
            "source": [
                "# Lets choose a random training sample\n",
                "idx = np.random.randint(0, len(labels)-1)\n",
                "\n",
                "# Using the index we call the input values \n",
                "# NOTE: This time, we will have to one-hot encode the input\n",
                "# in order to comply with our new model\n",
                "# Also, we will have to add one extra dimension to the input\n",
                "# using np.expand_dims in order to avoid warnings from tf.keras API\n",
                "onehot_target = np.expand_dims(to_categorical(word_target[idx],num_classes=vocab_size).reshape(1,-1),axis=0)\n",
                "onehot_context = np.expand_dims(to_categorical(word_context[idx],num_classes=vocab_size).reshape(1,-1),axis=0)\n",
                "training_label = np.array(labels[idx],dtype='float32').reshape(1,)\n",
                "\n",
                "# We use the tf.keras `model.train_on_batch` to train on a single batch\n",
                "# for demonstration that our model works\n",
                "loss = model.train_on_batch([onehot_target, onehot_context], training_label)\n",
                "print(f'Loss after one epoch is {loss:.2f}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Mindchow üç≤\n",
                "\n",
                "What do the weights in the dense layer of the above model signify?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_chow4) ###\n",
                "# Type your answer within in the quotes given\n",
                "answer4 = 'Each column in the embeddings dense weight matrix is the $n$-dimensional representation for each word in the vocabulary'"
            ]
        }
    ]
}
